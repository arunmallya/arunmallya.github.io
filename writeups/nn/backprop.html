
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
  <head>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Layout and HTML pieces the work of Jon Barron http://www.cs.berkeley.edu/~barron/ */
      a {
      color: #1772d1;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09227;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
    </style>

    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <title>Backprop</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">

  </head>

  <body>
    <table width="850" border="0" align="center" cellpadding="20">
      <tr>
	      <td> 

          <p align="center"><font size="6">Backpropagation</font></p>  
          <hr>

          <p> 
            <p align = "center"><b><a name="multiple_layers">Multiple Layers</a></b></p>
            Consider a network made of multiple layers, connected to some error function \( E(\cdot) \) at the top, as shown in the figure below.</br> 
            Each layer takes in some input, transforms it in some fashion, and then produces some output, which is then passed on to the next layer, until the error layer is reached. Once the error is computed with respect to the target, corrective gradients are passed down to change the network parameters.
            <p align = "center"><img width="20%" src="images/layers.png" alt="Current Input"></p>
          </p>
          <hr>

          <p> 
            <p align="center"><b> <a name="generic_layer">A Generic Layer</a></b></p>
            Most layers can be represented as applying some function \(f(\cdot)\) with parameters \(w\), on the input vector \(x\) of length \(m\) to produce output \(y\) of length \(n\). We'll use \(i\) to index values of \(x\) and \(j\) to index values of \(y\) in the parts that follow.</br> 
            Some layers might take in or produce multiple input/output values. A simple example is a loss layer which takes in input predictions and truth targets and then produces a corresponding error value.</br> 
            Some layers might not have any parameters \(w\) but instead just apply a fixed function. Examples of such layers include loss layers and non-linearity layers such as sigmoid, tanh, ReLU.
            <p align="center"><img width="30%" src="images/generic_layer.png" alt="Generic Layer"></p>
          </p>
          <hr>

          In the parts that follow, we'll look at some common layers and derive backpropagation equations for them.

          <hr>
          <p> 
            <p align="center"><b> <a name="l2_loss_layer">Euclidean Loss Layer</a></b></p>
            Consider the Euclidean Loss Layer. This takes in some input \(x\) and measures how far this input is from the expected targets \(t\). This layer technically produces the amount of error as its output, but loss layers are the end of a network, and its output is not passed on to any layer. So, our \(y\) here is non-existent. Also, this loss function is not parameterized by any weights \(w\).
            <p align="center"><img width="30%" src="images/l2_loss_layer.png" alt="Generic Layer"></p>
            <p align="center">
              \begin{align}
                \text{Error} = E &= \frac{1}{2}\sum_{i=1}^m (x_i - t_i)^2 \\
                \frac{\partial E}{\partial x_i} &= \frac{1}{2} \cdot 2 \cdot (x_i - t_i) = (x_i - t_i)\\
                \delta x &= \left[ \frac{\partial E}{\partial x_i} \right]_i = x - t
              \end{align}
              \(\delta x\) and \( t \) are of size \(m \times 1\)

              $$
                \boxed{
                  \begin{align}
                    \delta x &= x - t \\
                    \delta w &= \text{N.A.}
                  \end{align}
                }
              $$
            </p>
          </p>
          <hr>

          <p> 
            <p align="center"><b> <a name="sigmoid_layer">Sigmoid Layer</a></b></p>
            The sigmoid layer takes in some input \(x\) and applies the sigmoid function on each value in \(x\) to produce output \(y\). The sigmoid function too, does not have any parameters \(w\). It just applies a fixed transformation shown below, on each input value.
            $$
              y_i = \frac{1}{1+e^{-x_i}}
            $$
            <p align="center"><img width="30%" src="images/sigmoid_layer.png" alt="Sigmoid Layer"></p>
            <p align="center">
              \begin{align}
                \frac{\partial E}{\partial x_i} &= \frac{\partial E}{\partial y_i} \cdot \frac{\partial y_i}{\partial x_i} = \delta y_i \cdot \frac{\partial y_i}{\partial x_i}\\
                \frac{\partial y_i}{\partial x_i} &= \frac{e^{-x_i}}{(1+e^{-x_i})^2} = y_i \cdot (1-y_i)\\
                \therefore \frac{\partial E}{\partial x_i} &= \delta y_i \cdot y_i \cdot (1-y_i)\\
                \delta x &= \left[ \frac{\partial E}{\partial x_i} \right]_i = \delta y \odot y \odot (1-y)
              \end{align}
              Note that in this layer, \( m=n \)</br>
              \(\delta x\) is of size \(m \times 1\), \(\delta y\) is of size \(n \times 1\), \(y\) and \((1-y)\) are of size \(n \times 1\)

              $$
                \boxed{
                  \begin{align}
                    \delta x &= \delta y \odot y \odot (1-y) \\
                    \delta w &= \text{N.A.}
                  \end{align}
                }
              $$
            </p>
          </p>
          <hr>

          <p> 
            <p align="center"> <b> <a name="fc_layer">Fully Connected Layer</a></b></br></br> </p>
            
            In the fully connected layer, each output neuron \(y_1, \cdots, y_n \) is connected to each input value \(x_1, \cdots, x_m\) with a weight \(w_{ji}\).
            The collection of all weights, \( W = \{ w_{ji} \} \), is the set of parameters of this layer. \(W\) is a matrix of size \( n \times m \).
            The relationship between the inputs and outputs is shown below.</br>

            <p align="center">

              \( y_j = \displaystyle\sum_{i=1}^m w_{ji} \cdot x_i \) or equivalently in matrix form, \( y = W \times x \), where \(W = \left[ w_{ji} \right]\) is of size \( n \times m \)
              <img width="60%" src="images/fc_layer.png" alt="Current Input">
            
              $$
                \begin{align}
                  \frac{\partial E}{\partial x_{i}} &= \sum_{j=1}^n \left( \frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial x_i} \right) \\
                  \frac{\partial y_j}{\partial x_i} &= \frac{\partial \left( \displaystyle\sum_{i=1}^m w_{ji} \cdot x_i\right)}{\partial x_i} =  w_{ji}\\
                  \therefore \frac{\partial E}{\partial x_{i}} &= \sum_{j=1}^n \left( \delta y_j \cdot w_{ji} \right) = \sum_{j=1}^n \left( w^T_{ij} \cdot \delta y_j \right)\\
                  \delta x &= \left[ \frac{\partial E}{\partial x_{i}} \right]_i = W^T \times \delta y
                \end{align}
              $$
              \(\delta x\) is of size \(m \times 1\), \(W^T\) is of size \(m \times n\), \(y\) is of size \(n \times 1\)
              <hr width="40%">

              $$
                \begin{align}
                  \frac{\partial E}{\partial w_{ji}} &= \frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial w_{ji}} = \delta y_j \cdot \frac{\partial y_j}{\partial w_{ji}}\\
                  \frac{\partial y_j}{\partial w_{ji}} &= \frac{\partial \left( \displaystyle\sum_{i=1}^m w_{ji} \cdot x_i\right)}{\partial w_{ji}} =  x_i\\
                  \therefore \frac{\partial E}{\partial w_{ji}} &= \delta y_j \cdot x_i\\
                  \delta w &= \left[ \frac{\partial E}{\partial w_{ji}} \right]_{ji} = \delta y \times x^T \\
                \end{align}
              $$
              <p align="center">
                \(\delta w\) is of size \(n \times m\), \(\delta y\) is of size \(n \times 1\), \(x^T\) is of size \(1 \times m\)
              </p>

              $$
                \boxed{
                  \begin{align}
                    \delta x &= W^T \times \delta y \\
                    \delta w &= \delta y \times x^T 
                  \end{align}
                }
              $$
            </p>
          </p>
          <hr>

          </br></br></br></br></br></br></br></br>

        </td>
      </tr>
    </table>
  </body>
</html>
